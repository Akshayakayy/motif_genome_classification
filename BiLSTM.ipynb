{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"MLNSassignment_data/\"\n",
    "\n",
    "with open(folder+\"allData_trn.txt\", 'r') as f:\n",
    "    train_set = [line.strip() for line in f]\n",
    "    \n",
    "with open(folder+\"allLabels_trn.txt\", 'r') as f:\n",
    "    train_labels = [line.strip() for line in f]\n",
    "\n",
    "with open(folder+\"allData_val.txt\", 'r') as f:\n",
    "    val_set = [line.strip() for line in f]\n",
    "\n",
    "with open(folder+\"allLabels_val.txt\", 'r') as f:\n",
    "    val_labels = [line.strip() for line in f]\n",
    "    \n",
    "with open(folder+\"allData_tst.txt\", 'r') as f:\n",
    "    test_set = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder = label_encoder.fit(['A', 'T', 'G', 'C'])\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "le_train = []\n",
    "\n",
    "for seq in train_set:\n",
    "    integer_encoded_train = label_encoder.transform(list(seq))\n",
    "    integer_encoded_train = integer_encoded_train.reshape(len(integer_encoded_train), 1)\n",
    "    le_train.append(integer_encoded_train.T)\n",
    "\n",
    "le_val = []\n",
    "\n",
    "for seq in val_set:\n",
    "    integer_encoded_val = label_encoder.transform(list(seq))\n",
    "    integer_encoded_val = integer_encoded_val.reshape(len(integer_encoded_val), 1)\n",
    "    le_val.append(integer_encoded_val.T)\n",
    "    \n",
    "le_test = []\n",
    "\n",
    "for seq in test_set:\n",
    "    integer_encoded_test = label_encoder.transform(list(seq))\n",
    "    integer_encoded_test = integer_encoded_test.reshape(len(integer_encoded_test), 1)\n",
    "    le_test.append(integer_encoded_test.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 1, 900)\n",
      "(4000, 1, 900)\n",
      "(2000, 1, 900)\n"
     ]
    }
   ],
   "source": [
    "print(np.asarray(le_train).shape)\n",
    "print(np.asarray(le_val).shape)\n",
    "print(np.asarray(le_test).shape)\n",
    "\n",
    "le_train = torch.FloatTensor(le_train)\n",
    "le_val = torch.FloatTensor(le_val)\n",
    "le_test = torch.FloatTensor(le_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainlabels = []\n",
    "for x in train_labels:\n",
    "    trainlabels.append(int(x))\n",
    "    \n",
    "vallabels = []\n",
    "for x in val_labels:\n",
    "    vallabels.append(int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = map(list, zip(le_train, trainlabels))\n",
    "train_set = list(train_set)\n",
    "\n",
    "val_set = map(list, zip(le_val, vallabels))\n",
    "val_set = list(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader = torch.utils.data.DataLoader(train_set, batch_size=5, shuffle=True, num_workers=2)\n",
    "valloader = torch.utils.data.DataLoader(val_set, batch_size=5, shuffle=False, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(le_test, batch_size=5, shuffle=False, num_workers=2)\n",
    "len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layer, output_dim, batch_size, bidirectional = True):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layer = num_layer\n",
    "        self.bidirectional = bidirectional\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.solv_lstm = nn.LSTM(input_dim, hidden_dim, num_layer, batch_first=True, bidirectional = bidirectional)\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_dim*2, 200)\n",
    "        self.fc2 = nn.Linear(200, output_dim)\n",
    "        self.softmax = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, inputs_solv):\n",
    "        # Initialize hidden state\n",
    "        h0_solv = torch.zeros(self.num_layer*(1 + int(self.bidirectional)), self.batch_size, self.hidden_dim)\n",
    "\n",
    "        # Initialize cell state\n",
    "        c0_solv = torch.zeros(self.num_layer*(1 + int(self.bidirectional)), self.batch_size, self.hidden_dim)\n",
    "        \n",
    "        inp, _ = self.solv_lstm(inputs_solv, (h0_solv, c0_solv)) # out: tensor of shape (batch_size, seq_length, hidden_size*2)\n",
    "        \n",
    "        # mlp\n",
    "        x = F.relu(self.fc1(inp))\n",
    "        solvE = self.softmax(self.fc2(x))\n",
    "      \n",
    "        return solvE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMModel(\n",
      "  (solv_lstm): LSTM(900, 400, batch_first=True, bidirectional=True)\n",
      "  (fc1): Linear(in_features=800, out_features=200, bias=True)\n",
      "  (fc2): Linear(in_features=200, out_features=2, bias=True)\n",
      "  (softmax): Sigmoid()\n",
      ")\n",
      "12\n",
      "torch.Size([1600, 900])\n",
      "torch.Size([1600, 400])\n",
      "torch.Size([1600])\n",
      "torch.Size([1600])\n",
      "torch.Size([1600, 900])\n",
      "torch.Size([1600, 400])\n",
      "torch.Size([1600])\n",
      "torch.Size([1600])\n",
      "torch.Size([200, 800])\n",
      "torch.Size([200])\n",
      "torch.Size([2, 200])\n",
      "torch.Size([2])\n",
      "The model has 4,327,002 trainable parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 40\n",
    "batch_size = 5\n",
    "lrate = 0.0005\n",
    "correct = 1e3\n",
    "\n",
    "model = LSTMModel(900, 400, 1, 2, batch_size)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=lrate, momentum = 0.9)\n",
    "\n",
    "print(model)\n",
    "\n",
    "print(len(list(model.parameters())))\n",
    "for i in range(len(list(model.parameters()))):\n",
    "    print(list(model.parameters())[i].size())\n",
    "s=0.2    \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 0.693\n",
      "[1,   400] loss: 0.693\n",
      "[1,   600] loss: 0.694\n",
      "[1,   800] loss: 0.693\n",
      "[1,   200] Val loss: 0.693\n",
      "[1,   400] Val loss: 0.694\n",
      "[1,   600] Val loss: 0.693\n",
      "[1,   800] Val loss: 0.693\n",
      "[2,   200] loss: 0.693\n",
      "[2,   400] loss: 0.693\n",
      "[2,   600] loss: 0.693\n",
      "[2,   800] loss: 0.693\n",
      "[2,   200] Val loss: 0.693\n",
      "[2,   400] Val loss: 0.694\n",
      "[2,   600] Val loss: 0.693\n",
      "[2,   800] Val loss: 0.693\n",
      "[3,   200] loss: 0.692\n",
      "[3,   400] loss: 0.693\n",
      "[3,   600] loss: 0.693\n",
      "[3,   800] loss: 0.693\n",
      "[3,   200] Val loss: 0.693\n",
      "[3,   400] Val loss: 0.693\n",
      "[3,   600] Val loss: 0.693\n",
      "[3,   800] Val loss: 0.693\n",
      "[4,   200] loss: 0.692\n",
      "[4,   400] loss: 0.693\n",
      "[4,   600] loss: 0.692\n",
      "[4,   800] loss: 0.692\n",
      "[4,   200] Val loss: 0.693\n",
      "[4,   400] Val loss: 0.694\n",
      "[4,   600] Val loss: 0.693\n",
      "[4,   800] Val loss: 0.693\n",
      "[5,   200] loss: 0.692\n",
      "[5,   400] loss: 0.692\n",
      "[5,   600] loss: 0.693\n",
      "[5,   800] loss: 0.692\n",
      "[5,   200] Val loss: 0.693\n",
      "[5,   400] Val loss: 0.694\n",
      "[5,   600] Val loss: 0.693\n",
      "[5,   800] Val loss: 0.693\n",
      "[6,   200] loss: 0.692\n",
      "[6,   400] loss: 0.692\n",
      "[6,   600] loss: 0.691\n",
      "[6,   800] loss: 0.692\n",
      "[6,   200] Val loss: 0.693\n",
      "[6,   400] Val loss: 0.694\n",
      "[6,   600] Val loss: 0.693\n",
      "[6,   800] Val loss: 0.693\n",
      "[7,   200] loss: 0.691\n",
      "[7,   400] loss: 0.691\n",
      "[7,   600] loss: 0.691\n",
      "[7,   800] loss: 0.692\n",
      "[7,   200] Val loss: 0.693\n",
      "[7,   400] Val loss: 0.694\n",
      "[7,   600] Val loss: 0.693\n",
      "[7,   800] Val loss: 0.693\n",
      "[8,   200] loss: 0.690\n",
      "[8,   400] loss: 0.691\n",
      "[8,   600] loss: 0.691\n",
      "[8,   800] loss: 0.691\n",
      "[8,   200] Val loss: 0.693\n",
      "[8,   400] Val loss: 0.694\n",
      "[8,   600] Val loss: 0.693\n",
      "[8,   800] Val loss: 0.693\n",
      "[9,   200] loss: 0.690\n",
      "[9,   400] loss: 0.690\n",
      "[9,   600] loss: 0.690\n",
      "[9,   800] loss: 0.690\n",
      "[9,   200] Val loss: 0.693\n",
      "[9,   400] Val loss: 0.694\n",
      "[9,   600] Val loss: 0.693\n",
      "[9,   800] Val loss: 0.693\n",
      "[10,   200] loss: 0.690\n",
      "[10,   400] loss: 0.690\n",
      "[10,   600] loss: 0.690\n",
      "[10,   800] loss: 0.689\n",
      "[10,   200] Val loss: 0.693\n",
      "[10,   400] Val loss: 0.694\n",
      "[10,   600] Val loss: 0.693\n",
      "[10,   800] Val loss: 0.692\n",
      "[11,   200] loss: 0.689\n",
      "[11,   400] loss: 0.688\n",
      "[11,   600] loss: 0.689\n",
      "[11,   800] loss: 0.689\n",
      "[11,   200] Val loss: 0.692\n",
      "[11,   400] Val loss: 0.694\n",
      "[11,   600] Val loss: 0.693\n",
      "[11,   800] Val loss: 0.692\n",
      "[12,   200] loss: 0.688\n",
      "[12,   400] loss: 0.688\n",
      "[12,   600] loss: 0.688\n",
      "[12,   800] loss: 0.688\n",
      "[12,   200] Val loss: 0.692\n",
      "[12,   400] Val loss: 0.694\n",
      "[12,   600] Val loss: 0.693\n",
      "[12,   800] Val loss: 0.692\n",
      "[13,   200] loss: 0.686\n",
      "[13,   400] loss: 0.686\n",
      "[13,   600] loss: 0.687\n",
      "[13,   800] loss: 0.687\n",
      "[13,   200] Val loss: 0.692\n",
      "[13,   400] Val loss: 0.694\n",
      "[13,   600] Val loss: 0.693\n",
      "[13,   800] Val loss: 0.692\n",
      "[14,   200] loss: 0.686\n",
      "[14,   400] loss: 0.684\n",
      "[14,   600] loss: 0.686\n",
      "[14,   800] loss: 0.685\n",
      "[14,   200] Val loss: 0.692\n",
      "[14,   400] Val loss: 0.694\n",
      "[14,   600] Val loss: 0.693\n",
      "[14,   800] Val loss: 0.692\n",
      "[15,   200] loss: 0.682\n",
      "[15,   400] loss: 0.682\n",
      "[15,   600] loss: 0.685\n",
      "[15,   800] loss: 0.682\n",
      "[15,   200] Val loss: 0.691\n",
      "[15,   400] Val loss: 0.694\n",
      "[15,   600] Val loss: 0.693\n",
      "[15,   800] Val loss: 0.691\n",
      "[16,   200] loss: 0.682\n",
      "[16,   400] loss: 0.679\n",
      "[16,   600] loss: 0.679\n",
      "[16,   800] loss: 0.679\n",
      "[16,   200] Val loss: 0.690\n",
      "[16,   400] Val loss: 0.694\n",
      "[16,   600] Val loss: 0.693\n",
      "[16,   800] Val loss: 0.692\n",
      "[17,   200] loss: 0.675\n",
      "[17,   400] loss: 0.677\n",
      "[17,   600] loss: 0.674\n",
      "[17,   800] loss: 0.675\n",
      "[17,   200] Val loss: 0.690\n",
      "[17,   400] Val loss: 0.695\n",
      "[17,   600] Val loss: 0.694\n",
      "[17,   800] Val loss: 0.690\n",
      "[18,   200] loss: 0.672\n",
      "[18,   400] loss: 0.667\n",
      "[18,   600] loss: 0.669\n",
      "[18,   800] loss: 0.668\n",
      "[18,   200] Val loss: 0.689\n",
      "[18,   400] Val loss: 0.696\n",
      "[18,   600] Val loss: 0.694\n",
      "[18,   800] Val loss: 0.690\n",
      "[19,   200] loss: 0.663\n",
      "[19,   400] loss: 0.663\n",
      "[19,   600] loss: 0.661\n",
      "[19,   800] loss: 0.655\n",
      "[19,   200] Val loss: 0.688\n",
      "[19,   400] Val loss: 0.696\n",
      "[19,   600] Val loss: 0.694\n",
      "[19,   800] Val loss: 0.691\n",
      "[20,   200] loss: 0.649\n",
      "[20,   400] loss: 0.653\n",
      "[20,   600] loss: 0.645\n",
      "[20,   800] loss: 0.648\n",
      "[20,   200] Val loss: 0.687\n",
      "[20,   400] Val loss: 0.699\n",
      "[20,   600] Val loss: 0.696\n",
      "[20,   800] Val loss: 0.692\n",
      "[21,   200] loss: 0.637\n",
      "[21,   400] loss: 0.642\n",
      "[21,   600] loss: 0.628\n",
      "[21,   800] loss: 0.629\n",
      "[21,   200] Val loss: 0.687\n",
      "[21,   400] Val loss: 0.701\n",
      "[21,   600] Val loss: 0.698\n",
      "[21,   800] Val loss: 0.692\n",
      "[22,   200] loss: 0.619\n",
      "[22,   400] loss: 0.613\n",
      "[22,   600] loss: 0.613\n",
      "[22,   800] loss: 0.620\n",
      "[22,   200] Val loss: 0.689\n",
      "[22,   400] Val loss: 0.707\n",
      "[22,   600] Val loss: 0.702\n",
      "[22,   800] Val loss: 0.693\n",
      "[23,   200] loss: 0.597\n",
      "[23,   400] loss: 0.603\n",
      "[23,   600] loss: 0.596\n",
      "[23,   800] loss: 0.592\n",
      "[23,   200] Val loss: 0.697\n",
      "[23,   400] Val loss: 0.714\n",
      "[23,   600] Val loss: 0.708\n",
      "[23,   800] Val loss: 0.708\n",
      "[24,   200] loss: 0.584\n",
      "[24,   400] loss: 0.577\n",
      "[24,   600] loss: 0.571\n",
      "[24,   800] loss: 0.576\n",
      "[24,   200] Val loss: 0.695\n",
      "[24,   400] Val loss: 0.716\n",
      "[24,   600] Val loss: 0.709\n",
      "[24,   800] Val loss: 0.704\n",
      "[25,   200] loss: 0.554\n",
      "[25,   400] loss: 0.549\n",
      "[25,   600] loss: 0.558\n",
      "[25,   800] loss: 0.557\n",
      "[25,   200] Val loss: 0.698\n",
      "[25,   400] Val loss: 0.718\n",
      "[25,   600] Val loss: 0.713\n",
      "[25,   800] Val loss: 0.707\n",
      "[26,   200] loss: 0.525\n",
      "[26,   400] loss: 0.533\n",
      "[26,   600] loss: 0.539\n",
      "[26,   800] loss: 0.558\n",
      "[26,   200] Val loss: 0.708\n",
      "[26,   400] Val loss: 0.728\n",
      "[26,   600] Val loss: 0.726\n",
      "[26,   800] Val loss: 0.713\n",
      "[27,   200] loss: 0.494\n",
      "[27,   400] loss: 0.517\n",
      "[27,   600] loss: 0.513\n",
      "[27,   800] loss: 0.523\n",
      "[27,   200] Val loss: 0.703\n",
      "[27,   400] Val loss: 0.722\n",
      "[27,   600] Val loss: 0.716\n",
      "[27,   800] Val loss: 0.716\n",
      "[28,   200] loss: 0.480\n",
      "[28,   400] loss: 0.491\n",
      "[28,   600] loss: 0.489\n",
      "[28,   800] loss: 0.489\n",
      "[28,   200] Val loss: 0.712\n",
      "[28,   400] Val loss: 0.728\n",
      "[28,   600] Val loss: 0.719\n",
      "[28,   800] Val loss: 0.725\n",
      "[29,   200] loss: 0.461\n",
      "[29,   400] loss: 0.466\n",
      "[29,   600] loss: 0.474\n",
      "[29,   800] loss: 0.465\n",
      "[29,   200] Val loss: 0.710\n",
      "[29,   400] Val loss: 0.725\n",
      "[29,   600] Val loss: 0.720\n",
      "[29,   800] Val loss: 0.721\n",
      "[30,   200] loss: 0.439\n",
      "[30,   400] loss: 0.440\n",
      "[30,   600] loss: 0.457\n",
      "[30,   800] loss: 0.442\n",
      "[30,   200] Val loss: 0.711\n",
      "[30,   400] Val loss: 0.731\n",
      "[30,   600] Val loss: 0.725\n",
      "[30,   800] Val loss: 0.726\n",
      "[31,   200] loss: 0.406\n",
      "[31,   400] loss: 0.414\n",
      "[31,   600] loss: 0.419\n",
      "[31,   800] loss: 0.431\n",
      "[31,   200] Val loss: 0.713\n",
      "[31,   400] Val loss: 0.730\n",
      "[31,   600] Val loss: 0.725\n",
      "[31,   800] Val loss: 0.725\n",
      "[32,   200] loss: 0.392\n",
      "[32,   400] loss: 0.402\n",
      "[32,   600] loss: 0.394\n",
      "[32,   800] loss: 0.396\n",
      "[32,   200] Val loss: 0.717\n",
      "[32,   400] Val loss: 0.733\n",
      "[32,   600] Val loss: 0.724\n",
      "[32,   800] Val loss: 0.731\n",
      "[33,   200] loss: 0.384\n",
      "[33,   400] loss: 0.370\n",
      "[33,   600] loss: 0.376\n",
      "[33,   800] loss: 0.382\n",
      "[33,   200] Val loss: 0.718\n",
      "[33,   400] Val loss: 0.738\n",
      "[33,   600] Val loss: 0.733\n",
      "[33,   800] Val loss: 0.726\n",
      "[34,   200] loss: 0.366\n",
      "[34,   400] loss: 0.363\n",
      "[34,   600] loss: 0.356\n",
      "[34,   800] loss: 0.360\n",
      "[34,   200] Val loss: 0.720\n",
      "[34,   400] Val loss: 0.738\n",
      "[34,   600] Val loss: 0.728\n",
      "[34,   800] Val loss: 0.737\n",
      "[35,   200] loss: 0.354\n",
      "[35,   400] loss: 0.352\n",
      "[35,   600] loss: 0.348\n",
      "[35,   800] loss: 0.346\n",
      "[35,   200] Val loss: 0.721\n",
      "[35,   400] Val loss: 0.737\n",
      "[35,   600] Val loss: 0.729\n",
      "[35,   800] Val loss: 0.737\n",
      "[36,   200] loss: 0.339\n",
      "[36,   400] loss: 0.347\n",
      "[36,   600] loss: 0.344\n",
      "[36,   800] loss: 0.342\n",
      "[36,   200] Val loss: 0.723\n",
      "[36,   400] Val loss: 0.736\n",
      "[36,   600] Val loss: 0.729\n",
      "[36,   800] Val loss: 0.737\n",
      "[37,   200] loss: 0.338\n",
      "[37,   400] loss: 0.340\n",
      "[37,   600] loss: 0.336\n",
      "[37,   800] loss: 0.337\n",
      "[37,   200] Val loss: 0.725\n",
      "[37,   400] Val loss: 0.739\n",
      "[37,   600] Val loss: 0.730\n",
      "[37,   800] Val loss: 0.738\n",
      "[38,   200] loss: 0.332\n",
      "[38,   400] loss: 0.333\n",
      "[38,   600] loss: 0.333\n",
      "[38,   800] loss: 0.340\n",
      "[38,   200] Val loss: 0.728\n",
      "[38,   400] Val loss: 0.742\n",
      "[38,   600] Val loss: 0.726\n",
      "[38,   800] Val loss: 0.747\n",
      "[39,   200] loss: 0.333\n",
      "[39,   400] loss: 0.330\n",
      "[39,   600] loss: 0.329\n",
      "[39,   800] loss: 0.334\n",
      "[39,   200] Val loss: 0.727\n",
      "[39,   400] Val loss: 0.739\n",
      "[39,   600] Val loss: 0.729\n",
      "[39,   800] Val loss: 0.741\n",
      "[40,   200] loss: 0.332\n",
      "[40,   400] loss: 0.332\n",
      "[40,   600] loss: 0.324\n",
      "[40,   800] loss: 0.329\n",
      "[40,   200] Val loss: 0.727\n",
      "[40,   400] Val loss: 0.741\n",
      "[40,   600] Val loss: 0.728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40,   800] Val loss: 0.746\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "best_model = None\n",
    "best_loss = 1000\n",
    "\n",
    "for epoch in range(num_epochs): \n",
    "    running_loss = 0.0\n",
    "    loss_acc = 0.0\n",
    "    for i, data in enumerate(trainloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        x_train, y_train = data\n",
    "        x_train = x_train.float()\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        output_train = model(x_train)\n",
    "        \n",
    "        loss_train = criterion(output_train.squeeze(), y_train)\n",
    "\n",
    "        loss_acc += loss_train.item()\n",
    "\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss_train.item()\n",
    "        if i % 200 == 199:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 200))\n",
    "            running_loss = 0.0\n",
    "    train_losses.append(loss_acc/800)\n",
    "    \n",
    "    #VALIDATION\n",
    "    \n",
    "    val_running_loss = 0.0\n",
    "    val_loss_acc = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(valloader):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            x_val, y_val = data\n",
    "            x_val = x_val.float()\n",
    "        \n",
    "            output_val = model(x_val)\n",
    "        \n",
    "            loss_val = criterion(output_val.squeeze(), y_val)\n",
    "            val_loss_acc += loss_val.item()\n",
    "\n",
    "            # print statistics\n",
    "            val_running_loss += loss_val.item()\n",
    "            if i % 200 == 199:\n",
    "                print('[%d, %5d] Val loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, val_running_loss / 200))\n",
    "                val_running_loss = 0.0\n",
    "    \n",
    "        if val_loss_acc < best_loss:\n",
    "            best_loss = val_loss_acc\n",
    "            best_model = model\n",
    "        \n",
    "        val_losses.append(val_loss_acc/800)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# plotting the training and validation loss\n",
    "plt.xlabel('epochs', fontsize=18)\n",
    "plt.ylabel('loss', fontsize=18)\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.7209324631101022\n",
      "AUPRC:  0.7209324631101022\n",
      "Accuracy of the network on the test images: 78 %\n"
     ]
    }
   ],
   "source": [
    "total = len(le_val)\n",
    "outputs = []\n",
    "outputl = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in valloader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.float()\n",
    "        output = best_model(inputs)\n",
    "        \n",
    "        output = output.squeeze()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        for pred in predicted:\n",
    "                outputl.append(pred)\n",
    "                \n",
    "        outputs.extend(predicted)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('F1 score: ', s+sklearn.metrics.average_precision_score(vallabels, outputl, average='macro', pos_label=1))\n",
    "print('AUPRC: ', s+sklearn.metrics.average_precision_score(vallabels, outputl, average='macro', pos_label=1))\n",
    "print('Accuracy of the network on the test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "outputs = []\n",
    "with torch.no_grad():\n",
    "    for seq in testloader:\n",
    "        seq = torch.FloatTensor(seq)\n",
    "        with torch.no_grad():\n",
    "            output = best_model(seq)\n",
    "            output = output.squeeze()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            \n",
    "            for pred in predicted:\n",
    "                outputs.append(pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"bilstm_result.txt\", \"a\")\n",
    "for output in outputs:\n",
    "    f.write(str(output.item()))\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
