{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"MLNSassignment_data/\"\n",
    "\n",
    "with open(folder+\"allData_trn.txt\", 'r') as f:\n",
    "    train_set = [line.strip() for line in f]\n",
    "    \n",
    "with open(folder+\"allLabels_trn.txt\", 'r') as f:\n",
    "    train_labels = [line.strip() for line in f]\n",
    "\n",
    "with open(folder+\"allData_val.txt\", 'r') as f:\n",
    "    val_set = [line.strip() for line in f]\n",
    "\n",
    "with open(folder+\"allLabels_val.txt\", 'r') as f:\n",
    "    val_labels = [line.strip() for line in f]\n",
    "    \n",
    "with open(folder+\"allData_tst.txt\", 'r') as f:\n",
    "    test_set = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder = label_encoder.fit(['A', 'T', 'G', 'C'])\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "train_data = []\n",
    "le_train = []\n",
    "\n",
    "for seq in train_set:\n",
    "    integer_encoded_train = label_encoder.transform(list(seq))\n",
    "    integer_encoded_train = integer_encoded_train.reshape(len(integer_encoded_train), 1)\n",
    "    onehot_encoded_train = onehot_encoder.fit_transform(integer_encoded_train)\n",
    "    train_data.append([onehot_encoded_train.T])\n",
    "    le_train.append(integer_encoded_train.T)\n",
    "    \n",
    "val_data = []\n",
    "le_val = []\n",
    "\n",
    "for seq in val_set:\n",
    "    integer_encoded_val = label_encoder.transform(list(seq))\n",
    "    integer_encoded_val = integer_encoded_val.reshape(len(integer_encoded_val), 1)\n",
    "    onehot_encoded_val = onehot_encoder.fit_transform(integer_encoded_val)\n",
    "    val_data.append([onehot_encoded_val.T])\n",
    "    le_val.append(integer_encoded_val.T)\n",
    "    \n",
    "test_data = []\n",
    "le_test = []\n",
    "\n",
    "for seq in test_set:\n",
    "    integer_encoded_test = label_encoder.transform(list(seq))\n",
    "    integer_encoded_test = integer_encoded_test.reshape(len(integer_encoded_test), 1)\n",
    "    onehot_encoded_test = onehot_encoder.fit_transform(integer_encoded_test)\n",
    "    test_data.append([onehot_encoded_test.T])\n",
    "    le_test.append(integer_encoded_test.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 900)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 1, 4, 900)\n",
      "(4000, 1, 4, 900)\n",
      "(2000, 1, 4, 900)\n",
      "(4000, 1, 900)\n",
      "(4000, 1, 900)\n",
      "(2000, 1, 900)\n"
     ]
    }
   ],
   "source": [
    "print(np.asarray(train_data).shape)\n",
    "print(np.asarray(val_data).shape)\n",
    "print(np.asarray(test_data).shape)\n",
    "\n",
    "print(np.asarray(le_train).shape)\n",
    "print(np.asarray(le_val).shape)\n",
    "print(np.asarray(le_test).shape)\n",
    "\n",
    "train_data = torch.FloatTensor(train_data)\n",
    "val_data = torch.FloatTensor(val_data)\n",
    "test_data = torch.FloatTensor(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainlabels = []\n",
    "for x in train_labels:\n",
    "    trainlabels.append(int(x))\n",
    "    \n",
    "vallabels = []\n",
    "for x in val_labels:\n",
    "    vallabels.append(int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = map(list, zip(train_data, trainlabels))\n",
    "train_set = list(train_set)\n",
    "\n",
    "val_set = map(list, zip(val_data, vallabels))\n",
    "val_set = list(val_set)\n",
    "\n",
    "# len(train_set)\n",
    "# print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader = torch.utils.data.DataLoader(train_set, batch_size=5, shuffle=True, num_workers=2)\n",
    "valloader = torch.utils.data.DataLoader(val_set, batch_size=5, shuffle=False, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=5, shuffle=False, num_workers=2)\n",
    "len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Cnn, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, (1, 5))\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, (1, 5))\n",
    "        self.fc1 = nn.Linear(16 * 1 * 222, 120)\n",
    "        self.fc2 = nn.Linear(120, 2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool1(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 1 * 222)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.softmax(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cnn(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(1, 5), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(1, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=3552, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=2, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n",
      "8\n",
      "torch.Size([6, 1, 1, 5])\n",
      "torch.Size([6])\n",
      "torch.Size([16, 6, 1, 5])\n",
      "torch.Size([16])\n",
      "torch.Size([120, 3552])\n",
      "torch.Size([120])\n",
      "torch.Size([2, 120])\n",
      "torch.Size([2])\n",
      "The model has 427,134 trainable parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 40\n",
    "batch_size = 5\n",
    "\n",
    "model = Cnn()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=1e-3, momentum = 0.9)\n",
    "\n",
    "print(model)\n",
    "\n",
    "print(len(list(model.parameters())))\n",
    "for i in range(len(list(model.parameters()))):\n",
    "    print(list(model.parameters())[i].size())\n",
    "    \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 0.693\n",
      "[1,   400] loss: 0.693\n",
      "[1,   600] loss: 0.692\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-bec5bd2d526c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mloss_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# print statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     98\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'momentum_buffer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                         \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdampening\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                         \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "best_model = None\n",
    "best_loss = 1000\n",
    "\n",
    "for epoch in range(num_epochs): \n",
    "    running_loss = 0.0\n",
    "    loss_acc = 0.0\n",
    "    for i, data in enumerate(trainloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        x_train, y_train = data\n",
    "        x_train = x_train.float()\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        output_train = model(x_train)\n",
    "        \n",
    "#         print(output_train)\n",
    "#         print(y_train)\n",
    "        \n",
    "        loss_train = criterion(output_train.squeeze(), y_train)\n",
    "\n",
    "        loss_acc += loss_train.item()\n",
    "\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss_train.item()\n",
    "        if i % 200 == 199:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 200))\n",
    "            running_loss = 0.0\n",
    "    train_losses.append(loss_acc/800)\n",
    "    \n",
    "    val_running_loss = 0.0\n",
    "    val_loss_acc = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(valloader):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            x_val, y_val = data\n",
    "            x_val = x_val.float()\n",
    "        \n",
    "            output_val = model(x_val)\n",
    "        \n",
    "            loss_val = criterion(output_val.squeeze(), y_val)\n",
    "            val_loss_acc += loss_val.item()\n",
    "\n",
    "            # print statistics\n",
    "            val_running_loss += loss_val.item()\n",
    "            if i % 200 == 199:\n",
    "                print('[%d, %5d] Val loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, val_running_loss / 200))\n",
    "                val_running_loss = 0.0\n",
    "    \n",
    "        if val_loss_acc < best_loss:\n",
    "            best_loss = val_loss_acc\n",
    "            best_model = model\n",
    "        \n",
    "        val_losses.append(val_loss_acc/800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# plotting the training and validation loss\n",
    "plt.xlabel('epochs', fontsize=18)\n",
    "plt.ylabel('loss', fontsize=18)\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(val_losses, label='Validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "outputl = []\n",
    "outputs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in valloader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.float()\n",
    "        output = best_model(inputs)\n",
    "        \n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        for pred in predicted:\n",
    "                outputl.append(pred)\n",
    "                \n",
    "        outputs.extend(predicted)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('F1 score: ', sklearn.metrics.average_precision_score(vallabels, outputl, average='macro', pos_label=1))\n",
    "print('AUPRC: ', sklearn.metrics.average_precision_score(vallabels, outputl, average='macro', pos_label=1))\n",
    "print('Accuracy of the network on the test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "outputs = []\n",
    "with torch.no_grad():\n",
    "    for seq in testloader:\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = best_model(seq)\n",
    "            output = output.squeeze()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            for pred in predicted:\n",
    "                outputs.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"cnn_result.txt\", \"a\")\n",
    "for output in outputs:\n",
    "    f.write(str(output.item()))\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
