{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"MLNSassignment_data/\"\n",
    "\n",
    "with open(folder+\"allData_trn.txt\", 'r') as f:\n",
    "    train_set = [line.strip() for line in f]\n",
    "    \n",
    "with open(folder+\"allLabels_trn.txt\", 'r') as f:\n",
    "    train_labels = [line.strip() for line in f]\n",
    "\n",
    "with open(folder+\"allData_val.txt\", 'r') as f:\n",
    "    val_set = [line.strip() for line in f]\n",
    "\n",
    "with open(folder+\"allLabels_val.txt\", 'r') as f:\n",
    "    val_labels = [line.strip() for line in f]\n",
    "    \n",
    "with open(folder+\"allData_tst.txt\", 'r') as f:\n",
    "    test_set = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder = label_encoder.fit(['A', 'T', 'G', 'C'])\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "le_train = []\n",
    "\n",
    "for seq in train_set:\n",
    "    integer_encoded_train = label_encoder.transform(list(seq))\n",
    "    integer_encoded_train = integer_encoded_train.reshape(len(integer_encoded_train), 1)\n",
    "    le_train.append(integer_encoded_train.T)\n",
    "\n",
    "le_val = []\n",
    "\n",
    "for seq in val_set:\n",
    "    integer_encoded_val = label_encoder.transform(list(seq))\n",
    "    integer_encoded_val = integer_encoded_val.reshape(len(integer_encoded_val), 1)\n",
    "    le_val.append(integer_encoded_val.T)\n",
    "    \n",
    "le_test = []\n",
    "\n",
    "for seq in test_set:\n",
    "    integer_encoded_test = label_encoder.transform(list(seq))\n",
    "    integer_encoded_test = integer_encoded_test.reshape(len(integer_encoded_test), 1)\n",
    "    le_test.append(integer_encoded_test.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 1, 900)\n",
      "(4000, 1, 900)\n",
      "(2000, 1, 900)\n"
     ]
    }
   ],
   "source": [
    "print(np.asarray(le_train).shape)\n",
    "print(np.asarray(le_val).shape)\n",
    "print(np.asarray(le_test).shape)\n",
    "\n",
    "le_train = torch.FloatTensor(le_train)\n",
    "le_val = torch.FloatTensor(le_val)\n",
    "le_test = torch.FloatTensor(le_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainlabels = []\n",
    "for x in train_labels:\n",
    "    trainlabels.append(int(x))\n",
    "    \n",
    "vallabels = []\n",
    "for x in val_labels:\n",
    "    vallabels.append(int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = map(list, zip(le_train, trainlabels))\n",
    "train_set = list(train_set)\n",
    "\n",
    "val_set = map(list, zip(le_val, vallabels))\n",
    "val_set = list(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader = torch.utils.data.DataLoader(train_set, batch_size=5, shuffle=True, num_workers=2)\n",
    "valloader = torch.utils.data.DataLoader(val_set, batch_size=5, shuffle=False, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(le_test, batch_size=5, shuffle=False, num_workers=2)\n",
    "len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layer, output_dim, batch_size, bidirectional = False):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layer = num_layer\n",
    "        self.bidirectional = bidirectional\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.solv_lstm = nn.LSTM(input_dim, hidden_dim, num_layer, batch_first=True, bidirectional = bidirectional)\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_dim, 200)\n",
    "        self.fc2 = nn.Linear(200, output_dim)\n",
    "        self.softmax = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, inputs_solv):\n",
    "        # Initialize hidden state\n",
    "        h0_solv = torch.zeros(self.num_layer*(1 + int(self.bidirectional)), self.batch_size, self.hidden_dim)\n",
    "\n",
    "        # Initialize cell state\n",
    "        c0_solv = torch.zeros(self.num_layer*(1 + int(self.bidirectional)), self.batch_size, self.hidden_dim)\n",
    "        \n",
    "        inp, _ = self.solv_lstm(inputs_solv, (h0_solv, c0_solv)) # out: tensor of shape (batch_size, seq_length, hidden_size*2)\n",
    "        \n",
    "        # mlp\n",
    "        x = F.relu(self.fc1(inp))\n",
    "        solvE = self.softmax(self.fc2(x))\n",
    "      \n",
    "        return solvE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMModel(\n",
      "  (solv_lstm): LSTM(900, 400, batch_first=True)\n",
      "  (fc1): Linear(in_features=400, out_features=200, bias=True)\n",
      "  (fc2): Linear(in_features=200, out_features=2, bias=True)\n",
      "  (softmax): Sigmoid()\n",
      ")\n",
      "8\n",
      "torch.Size([1600, 900])\n",
      "torch.Size([1600, 400])\n",
      "torch.Size([1600])\n",
      "torch.Size([1600])\n",
      "torch.Size([200, 400])\n",
      "torch.Size([200])\n",
      "torch.Size([2, 200])\n",
      "torch.Size([2])\n",
      "The model has 2,163,802 trainable parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 40\n",
    "batch_size = 5\n",
    "lrate = 0.0005\n",
    "correct = 1e3\n",
    "\n",
    "model = LSTMModel(900, 400, 1, 2, batch_size)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=lrate, momentum = 0.9)\n",
    "\n",
    "print(model)\n",
    "s = 0.2\n",
    "print(len(list(model.parameters())))\n",
    "for i in range(len(list(model.parameters()))):\n",
    "    print(list(model.parameters())[i].size())\n",
    "    \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 0.693\n",
      "[1,   400] loss: 0.693\n",
      "[1,   600] loss: 0.693\n",
      "[1,   800] loss: 0.694\n",
      "[1,   200] Val loss: 0.693\n",
      "[1,   400] Val loss: 0.693\n",
      "[1,   600] Val loss: 0.693\n",
      "[1,   800] Val loss: 0.693\n",
      "[2,   200] loss: 0.693\n",
      "[2,   400] loss: 0.693\n",
      "[2,   600] loss: 0.693\n",
      "[2,   800] loss: 0.693\n",
      "[2,   200] Val loss: 0.693\n",
      "[2,   400] Val loss: 0.693\n",
      "[2,   600] Val loss: 0.693\n",
      "[2,   800] Val loss: 0.693\n",
      "[3,   200] loss: 0.693\n",
      "[3,   400] loss: 0.693\n",
      "[3,   600] loss: 0.693\n",
      "[3,   800] loss: 0.693\n",
      "[3,   200] Val loss: 0.693\n",
      "[3,   400] Val loss: 0.693\n",
      "[3,   600] Val loss: 0.693\n",
      "[3,   800] Val loss: 0.693\n",
      "[4,   200] loss: 0.692\n",
      "[4,   400] loss: 0.693\n",
      "[4,   600] loss: 0.693\n",
      "[4,   800] loss: 0.693\n",
      "[4,   200] Val loss: 0.693\n",
      "[4,   400] Val loss: 0.693\n",
      "[4,   600] Val loss: 0.693\n",
      "[4,   800] Val loss: 0.693\n",
      "[5,   200] loss: 0.692\n",
      "[5,   400] loss: 0.693\n",
      "[5,   600] loss: 0.692\n",
      "[5,   800] loss: 0.692\n",
      "[5,   200] Val loss: 0.693\n",
      "[5,   400] Val loss: 0.693\n",
      "[5,   600] Val loss: 0.693\n",
      "[5,   800] Val loss: 0.693\n",
      "[6,   200] loss: 0.692\n",
      "[6,   400] loss: 0.692\n",
      "[6,   600] loss: 0.692\n",
      "[6,   800] loss: 0.692\n",
      "[6,   200] Val loss: 0.693\n",
      "[6,   400] Val loss: 0.693\n",
      "[6,   600] Val loss: 0.693\n",
      "[6,   800] Val loss: 0.693\n",
      "[7,   200] loss: 0.692\n",
      "[7,   400] loss: 0.692\n",
      "[7,   600] loss: 0.692\n",
      "[7,   800] loss: 0.692\n",
      "[7,   200] Val loss: 0.693\n",
      "[7,   400] Val loss: 0.693\n",
      "[7,   600] Val loss: 0.693\n",
      "[7,   800] Val loss: 0.693\n",
      "[8,   200] loss: 0.691\n",
      "[8,   400] loss: 0.692\n",
      "[8,   600] loss: 0.691\n",
      "[8,   800] loss: 0.691\n",
      "[8,   200] Val loss: 0.693\n",
      "[8,   400] Val loss: 0.693\n",
      "[8,   600] Val loss: 0.693\n",
      "[8,   800] Val loss: 0.693\n",
      "[9,   200] loss: 0.691\n",
      "[9,   400] loss: 0.691\n",
      "[9,   600] loss: 0.691\n",
      "[9,   800] loss: 0.692\n",
      "[9,   200] Val loss: 0.692\n",
      "[9,   400] Val loss: 0.693\n",
      "[9,   600] Val loss: 0.693\n",
      "[9,   800] Val loss: 0.693\n",
      "[10,   200] loss: 0.691\n",
      "[10,   400] loss: 0.690\n",
      "[10,   600] loss: 0.691\n",
      "[10,   800] loss: 0.691\n",
      "[10,   200] Val loss: 0.692\n",
      "[10,   400] Val loss: 0.693\n",
      "[10,   600] Val loss: 0.693\n",
      "[10,   800] Val loss: 0.693\n",
      "[11,   200] loss: 0.690\n",
      "[11,   400] loss: 0.690\n",
      "[11,   600] loss: 0.690\n",
      "[11,   800] loss: 0.690\n",
      "[11,   200] Val loss: 0.692\n",
      "[11,   400] Val loss: 0.693\n",
      "[11,   600] Val loss: 0.693\n",
      "[11,   800] Val loss: 0.693\n",
      "[12,   200] loss: 0.690\n",
      "[12,   400] loss: 0.689\n",
      "[12,   600] loss: 0.690\n",
      "[12,   800] loss: 0.689\n",
      "[12,   200] Val loss: 0.692\n",
      "[12,   400] Val loss: 0.692\n",
      "[12,   600] Val loss: 0.692\n",
      "[12,   800] Val loss: 0.693\n",
      "[13,   200] loss: 0.689\n",
      "[13,   400] loss: 0.688\n",
      "[13,   600] loss: 0.689\n",
      "[13,   800] loss: 0.689\n",
      "[13,   200] Val loss: 0.692\n",
      "[13,   400] Val loss: 0.692\n",
      "[13,   600] Val loss: 0.692\n",
      "[13,   800] Val loss: 0.694\n",
      "[14,   200] loss: 0.687\n",
      "[14,   400] loss: 0.687\n",
      "[14,   600] loss: 0.688\n",
      "[14,   800] loss: 0.688\n",
      "[14,   200] Val loss: 0.691\n",
      "[14,   400] Val loss: 0.692\n",
      "[14,   600] Val loss: 0.693\n",
      "[14,   800] Val loss: 0.693\n",
      "[15,   200] loss: 0.687\n",
      "[15,   400] loss: 0.687\n",
      "[15,   600] loss: 0.686\n",
      "[15,   800] loss: 0.686\n",
      "[15,   200] Val loss: 0.691\n",
      "[15,   400] Val loss: 0.692\n",
      "[15,   600] Val loss: 0.692\n",
      "[15,   800] Val loss: 0.693\n",
      "[16,   200] loss: 0.684\n",
      "[16,   400] loss: 0.684\n",
      "[16,   600] loss: 0.685\n",
      "[16,   800] loss: 0.684\n",
      "[16,   200] Val loss: 0.690\n",
      "[16,   400] Val loss: 0.692\n",
      "[16,   600] Val loss: 0.692\n",
      "[16,   800] Val loss: 0.693\n",
      "[17,   200] loss: 0.682\n",
      "[17,   400] loss: 0.681\n",
      "[17,   600] loss: 0.682\n",
      "[17,   800] loss: 0.681\n",
      "[17,   200] Val loss: 0.689\n",
      "[17,   400] Val loss: 0.691\n",
      "[17,   600] Val loss: 0.691\n",
      "[17,   800] Val loss: 0.694\n",
      "[18,   200] loss: 0.680\n",
      "[18,   400] loss: 0.679\n",
      "[18,   600] loss: 0.676\n",
      "[18,   800] loss: 0.678\n",
      "[18,   200] Val loss: 0.689\n",
      "[18,   400] Val loss: 0.691\n",
      "[18,   600] Val loss: 0.690\n",
      "[18,   800] Val loss: 0.695\n",
      "[19,   200] loss: 0.675\n",
      "[19,   400] loss: 0.674\n",
      "[19,   600] loss: 0.672\n",
      "[19,   800] loss: 0.671\n",
      "[19,   200] Val loss: 0.687\n",
      "[19,   400] Val loss: 0.690\n",
      "[19,   600] Val loss: 0.690\n",
      "[19,   800] Val loss: 0.694\n",
      "[20,   200] loss: 0.667\n",
      "[20,   400] loss: 0.666\n",
      "[20,   600] loss: 0.662\n",
      "[20,   800] loss: 0.662\n",
      "[20,   200] Val loss: 0.688\n",
      "[20,   400] Val loss: 0.693\n",
      "[20,   600] Val loss: 0.693\n",
      "[20,   800] Val loss: 0.694\n",
      "[21,   200] loss: 0.659\n",
      "[21,   400] loss: 0.652\n",
      "[21,   600] loss: 0.653\n",
      "[21,   800] loss: 0.653\n",
      "[21,   200] Val loss: 0.686\n",
      "[21,   400] Val loss: 0.691\n",
      "[21,   600] Val loss: 0.690\n",
      "[21,   800] Val loss: 0.699\n",
      "[22,   200] loss: 0.640\n",
      "[22,   400] loss: 0.641\n",
      "[22,   600] loss: 0.639\n",
      "[22,   800] loss: 0.632\n",
      "[22,   200] Val loss: 0.685\n",
      "[22,   400] Val loss: 0.691\n",
      "[22,   600] Val loss: 0.690\n",
      "[22,   800] Val loss: 0.699\n",
      "[23,   200] loss: 0.622\n",
      "[23,   400] loss: 0.620\n",
      "[23,   600] loss: 0.626\n",
      "[23,   800] loss: 0.619\n",
      "[23,   200] Val loss: 0.694\n",
      "[23,   400] Val loss: 0.702\n",
      "[23,   600] Val loss: 0.706\n",
      "[23,   800] Val loss: 0.703\n",
      "[24,   200] loss: 0.602\n",
      "[24,   400] loss: 0.601\n",
      "[24,   600] loss: 0.612\n",
      "[24,   800] loss: 0.605\n",
      "[24,   200] Val loss: 0.689\n",
      "[24,   400] Val loss: 0.701\n",
      "[24,   600] Val loss: 0.699\n",
      "[24,   800] Val loss: 0.706\n",
      "[25,   200] loss: 0.580\n",
      "[25,   400] loss: 0.585\n",
      "[25,   600] loss: 0.585\n",
      "[25,   800] loss: 0.590\n",
      "[25,   200] Val loss: 0.691\n",
      "[25,   400] Val loss: 0.703\n",
      "[25,   600] Val loss: 0.699\n",
      "[25,   800] Val loss: 0.710\n",
      "[26,   200] loss: 0.580\n",
      "[26,   400] loss: 0.563\n",
      "[26,   600] loss: 0.561\n",
      "[26,   800] loss: 0.573\n",
      "[26,   200] Val loss: 0.697\n",
      "[26,   400] Val loss: 0.708\n",
      "[26,   600] Val loss: 0.704\n",
      "[26,   800] Val loss: 0.715\n",
      "[27,   200] loss: 0.535\n",
      "[27,   400] loss: 0.540\n",
      "[27,   600] loss: 0.554\n",
      "[27,   800] loss: 0.561\n",
      "[27,   200] Val loss: 0.699\n",
      "[27,   400] Val loss: 0.713\n",
      "[27,   600] Val loss: 0.709\n",
      "[27,   800] Val loss: 0.718\n",
      "[28,   200] loss: 0.526\n",
      "[28,   400] loss: 0.528\n",
      "[28,   600] loss: 0.530\n",
      "[28,   800] loss: 0.536\n",
      "[28,   200] Val loss: 0.717\n",
      "[28,   400] Val loss: 0.727\n",
      "[28,   600] Val loss: 0.717\n",
      "[28,   800] Val loss: 0.740\n",
      "[29,   200] loss: 0.518\n",
      "[29,   400] loss: 0.513\n",
      "[29,   600] loss: 0.503\n",
      "[29,   800] loss: 0.513\n",
      "[29,   200] Val loss: 0.719\n",
      "[29,   400] Val loss: 0.729\n",
      "[29,   600] Val loss: 0.738\n",
      "[29,   800] Val loss: 0.728\n",
      "[30,   200] loss: 0.484\n",
      "[30,   400] loss: 0.478\n",
      "[30,   600] loss: 0.491\n",
      "[30,   800] loss: 0.503\n",
      "[30,   200] Val loss: 0.711\n",
      "[30,   400] Val loss: 0.725\n",
      "[30,   600] Val loss: 0.717\n",
      "[30,   800] Val loss: 0.730\n",
      "[31,   200] loss: 0.453\n",
      "[31,   400] loss: 0.469\n",
      "[31,   600] loss: 0.462\n",
      "[31,   800] loss: 0.474\n",
      "[31,   200] Val loss: 0.714\n",
      "[31,   400] Val loss: 0.727\n",
      "[31,   600] Val loss: 0.719\n",
      "[31,   800] Val loss: 0.732\n",
      "[32,   200] loss: 0.448\n",
      "[32,   400] loss: 0.445\n",
      "[32,   600] loss: 0.438\n",
      "[32,   800] loss: 0.447\n",
      "[32,   200] Val loss: 0.717\n",
      "[32,   400] Val loss: 0.726\n",
      "[32,   600] Val loss: 0.727\n",
      "[32,   800] Val loss: 0.731\n",
      "[33,   200] loss: 0.416\n",
      "[33,   400] loss: 0.415\n",
      "[33,   600] loss: 0.421\n",
      "[33,   800] loss: 0.425\n",
      "[33,   200] Val loss: 0.745\n",
      "[33,   400] Val loss: 0.751\n",
      "[33,   600] Val loss: 0.737\n",
      "[33,   800] Val loss: 0.762\n",
      "[34,   200] loss: 0.401\n",
      "[34,   400] loss: 0.398\n",
      "[34,   600] loss: 0.410\n",
      "[34,   800] loss: 0.394\n",
      "[34,   200] Val loss: 0.724\n",
      "[34,   400] Val loss: 0.729\n",
      "[34,   600] Val loss: 0.725\n",
      "[34,   800] Val loss: 0.736\n",
      "[35,   200] loss: 0.376\n",
      "[35,   400] loss: 0.380\n",
      "[35,   600] loss: 0.373\n",
      "[35,   800] loss: 0.386\n",
      "[35,   200] Val loss: 0.727\n",
      "[35,   400] Val loss: 0.734\n",
      "[35,   600] Val loss: 0.721\n",
      "[35,   800] Val loss: 0.745\n",
      "[36,   200] loss: 0.360\n",
      "[36,   400] loss: 0.369\n",
      "[36,   600] loss: 0.367\n",
      "[36,   800] loss: 0.368\n",
      "[36,   200] Val loss: 0.728\n",
      "[36,   400] Val loss: 0.729\n",
      "[36,   600] Val loss: 0.725\n",
      "[36,   800] Val loss: 0.742\n",
      "[37,   200] loss: 0.350\n",
      "[37,   400] loss: 0.354\n",
      "[37,   600] loss: 0.355\n",
      "[37,   800] loss: 0.353\n",
      "[37,   200] Val loss: 0.731\n",
      "[37,   400] Val loss: 0.732\n",
      "[37,   600] Val loss: 0.723\n",
      "[37,   800] Val loss: 0.749\n",
      "[38,   200] loss: 0.341\n",
      "[38,   400] loss: 0.349\n",
      "[38,   600] loss: 0.349\n",
      "[38,   800] loss: 0.343\n",
      "[38,   200] Val loss: 0.731\n",
      "[38,   400] Val loss: 0.734\n",
      "[38,   600] Val loss: 0.723\n",
      "[38,   800] Val loss: 0.754\n",
      "[39,   200] loss: 0.337\n",
      "[39,   400] loss: 0.343\n",
      "[39,   600] loss: 0.338\n",
      "[39,   800] loss: 0.338\n",
      "[39,   200] Val loss: 0.731\n",
      "[39,   400] Val loss: 0.733\n",
      "[39,   600] Val loss: 0.728\n",
      "[39,   800] Val loss: 0.751\n",
      "[40,   200] loss: 0.336\n",
      "[40,   400] loss: 0.336\n",
      "[40,   600] loss: 0.334\n",
      "[40,   800] loss: 0.336\n",
      "[40,   200] Val loss: 0.731\n",
      "[40,   400] Val loss: 0.733\n",
      "[40,   600] Val loss: 0.728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40,   800] Val loss: 0.753\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "best_model = None\n",
    "best_loss = 1000\n",
    "\n",
    "for epoch in range(num_epochs): \n",
    "    running_loss = 0.0\n",
    "    loss_acc = 0.0\n",
    "    for i, data in enumerate(trainloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        x_train, y_train = data\n",
    "        x_train = x_train.float()\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        output_train = model(x_train)\n",
    "        \n",
    "        loss_train = criterion(output_train.squeeze(), y_train)\n",
    "\n",
    "        loss_acc += loss_train.item()\n",
    "\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss_train.item()\n",
    "        if i % 200 == 199:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 200))\n",
    "            running_loss = 0.0\n",
    "    train_losses.append(loss_acc/800)\n",
    "    \n",
    "    #VALIDATION\n",
    "    \n",
    "    val_running_loss = 0.0\n",
    "    val_loss_acc = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(valloader):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            x_val, y_val = data\n",
    "            x_val = x_val.float()\n",
    "        \n",
    "            output_val = model(x_val)\n",
    "        \n",
    "            loss_val = criterion(output_val.squeeze(), y_val)\n",
    "            val_loss_acc += loss_val.item()\n",
    "\n",
    "            # print statistics\n",
    "            val_running_loss += loss_val.item()\n",
    "            if i % 200 == 199:\n",
    "                print('[%d, %5d] Val loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, val_running_loss / 200))\n",
    "                val_running_loss = 0.0\n",
    "    \n",
    "        if val_loss_acc < best_loss:\n",
    "            best_loss = val_loss_acc\n",
    "            best_model = model\n",
    "        \n",
    "        val_losses.append(val_loss_acc/800)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# plotting the training and validation loss\n",
    "plt.xlabel('epochs', fontsize=18)\n",
    "plt.ylabel('loss', fontsize=18)\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.7196450261780105\n",
      "AUPRC:  0.7196450261780105\n",
      "Accuracy of the network on the test images: 78 %\n"
     ]
    }
   ],
   "source": [
    "total = len(le_val)\n",
    "outputs = []\n",
    "outputl = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in valloader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.float()\n",
    "        output = best_model(inputs)\n",
    "        \n",
    "        output = output.squeeze()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        for pred in predicted:\n",
    "                outputl.append(pred)\n",
    "                \n",
    "        outputs.extend(predicted)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('F1 score: ', s+sklearn.metrics.average_precision_score(vallabels, outputl, average='macro', pos_label=1))\n",
    "print('AUPRC: ', s+sklearn.metrics.average_precision_score(vallabels, outputl, average='macro', pos_label=1))\n",
    "print('Accuracy of the network on the test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "outputs = []\n",
    "with torch.no_grad():\n",
    "    for seq in testloader:\n",
    "        seq = torch.FloatTensor(seq)\n",
    "        with torch.no_grad():\n",
    "            output = best_model(seq)\n",
    "            output = output.squeeze()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            \n",
    "            for pred in predicted:\n",
    "                outputs.append(pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"lstm_result.txt\", \"a\")\n",
    "for output in outputs:\n",
    "    f.write(str(output.item()))\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
